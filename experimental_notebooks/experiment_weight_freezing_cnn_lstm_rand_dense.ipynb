{"cells":[{"cell_type":"markdown","source":["# A brief experiment to investigate the sources of random wiring performance benefits/deficits in the CNN-LSTM model\n","See Section 6c of the manuscript"],"metadata":{"id":"7wGc0SBwVu_D"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1036,"status":"ok","timestamp":1665086205146,"user":{"displayName":"William Yik","userId":"05457595871295349459"},"user_tz":420},"id":"gaaSsXGFLbR9","outputId":"97355fb5-d8d2-414b-f961-838680b64c8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/Colab Notebooks/USC Random NN')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3906,"status":"ok","timestamp":1665086209047,"user":{"displayName":"William Yik","userId":"05457595871295349459"},"user_tz":420},"id":"H_b2Lolj3r-u","outputId":"622e4785-299f-4d7d-b5e7-9dc4764daf5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting importlib-metadata==4.13.0\n","  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n","Collecting typing-extensions>=3.6.4\n","  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n","Collecting zipp>=0.5\n","  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n","Installing collected packages: zipp, typing-extensions, importlib-metadata\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.1.2 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n","spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n","confection 0.0.2 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\u001b[0m\n","Successfully installed importlib-metadata-5.0.0 typing-extensions-4.3.0 zipp-3.8.1\n"]}],"source":["# Some xarray functions are currently broken with version 5.0.0\n","!pip install -I importlib-metadata==4.13.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARewm685LjdA"},"outputs":[],"source":["!pip install eofs --quiet\n","import numpy as np\n","import xarray as xr\n","import pandas as pd\n","from utils import *\n","from utils_cnn_lstm import *\n","results_path = results_path + 'weight_freezing/cnn_lstm_rand_dense/'\n","\n","from random_nn import *\n","import keras\n","from keras import Sequential, Model\n","from keras.layers import *\n","from keras.callbacks import EarlyStopping\n","import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","tf.get_logger().setLevel('ERROR')\n","import absl.logging\n","absl.logging.set_verbosity(absl.logging.ERROR)\n","from IPython.display import display\n","\n","from scipy import stats\n","\n","tf.keras.utils.set_random_seed(21)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38130,"status":"ok","timestamp":1665086253074,"user":{"displayName":"William Yik","userId":"05457595871295349459"},"user_tz":420},"id":"MULw8D9FVb4F","outputId":"682f919b-9cd0-4bd0-85b3-81920abd4b49"},"outputs":[{"data":{"text/plain":["((608, 10, 96, 144, 4),\n"," (608, 1, 96, 144),\n"," (118, 10, 96, 144, 4),\n"," (118, 1, 96, 144))"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["vars_to_predict = ['tas', 'diurnal_temperature_range', 'pr', 'pr90']\n","simus = ['ssp126',\n","         'ssp370',\n","         'ssp585',\n","         'hist-GHG',\n","         'hist-aer']\n","slider = 10  # sliding time window\n","\n","# Selects all of 2081-2100 data as validation\n","#val_idx = list(range(75,86)) + list(range(161,172)) + list(range(403,414))\n","\n","# Selects first two years of every decade from 2050 onward as validation\n","'''\n","val_idx = np.concatenate((np.arange(44,85,10), np.arange(45,86,10),\n","                          np.arange(130,171,10), np.arange(131,172,10), \n","                          np.arange(372,413,10), np.arange(373,414,10)))\n","'''\n","\n","# Selects first three years of every decade from 2050 onward as validation\n","'''\n","val_idx = np.concatenate((np.arange(44,85,10), np.arange(45,86,10), np.arange(46,77,10),\n","                          np.arange(130,171,10), np.arange(131,172,10), np.arange(132,163,10),\n","                          np.arange(372,413,10), np.arange(373,414,10), np.arange(374,405,10)))\n","'''\n","\n","# Selects first two years of every decade from 2020 onward as validation\n","'''\n","val_idx = np.concatenate((np.arange(14,85,10), np.arange(15,86,10),\n","                          np.arange(100,171,10), np.arange(101,172,10), \n","                          np.arange(342,413,10), np.arange(343,414,10)))\n","'''\n","\n","# Selects first two years of every decade from 1850 onward as validation\n","val_idx = np.concatenate((np.arange(4,85,10), np.arange(5,86,10),\n","                          np.arange(90,171,10), np.arange(91,172,10), \n","                          np.arange(332,413,10), np.arange(333,414,10),\n","                          np.arange(414,565,10), np.arange(415,566,10),\n","                          np.arange(570,721,10), np.arange(571,722,10)))\n","\n","# Selects continuous chunk of data from 2000s as validation\n","#val_idx = np.concatenate((np.arange(0,45), np.arange(86,131), np.arange(328,373)))\n","\n","X_train_dict = {}\n","Y_train_dict = {}\n","X_val_dict = {}\n","Y_val_dict = {}\n","\n","# Create training data\n","for var in vars_to_predict:\n","  X, Y, meanstd_inputs = create_training_data(simus, var_to_predict=var)\n","    \n","  X_val = np.take(X, val_idx, axis=0)\n","  X_train = np.delete(X, val_idx, axis=0)\n","  Y_val = np.take(Y, val_idx, axis=0)\n","  Y_train = np.delete(Y, val_idx, axis=0)\n","    \n","  X_train_dict[var] = X_train\n","  X_val_dict[var] = X_val\n","  Y_train_dict[var] = Y_train\n","  Y_val_dict[var] = Y_val\n","\n","# Open, reformat, and normalize test data\n","X_test = xr.open_mfdataset([data_path + 'inputs_historical.nc',\n","                            data_path + 'inputs_ssp245.nc']).compute()\n","Y_test = create_predictdand_data(['ssp245'])\n","\n","for input_var in ['CO2', 'CH4', 'SO2', 'BC']: \n","  var_dims = X_test[input_var].dims\n","  X_test = X_test.assign({input_var: (var_dims, normalize(X_test[input_var].data, input_var, meanstd_inputs))}) \n","    \n","X_test_np = input_for_training(X_test, skip_historical=False, len_historical=165) \n","\n","X_train_dict['tas'].shape, Y_train_dict['tas'].shape, X_val_dict['tas'].shape, Y_val_dict['tas'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfTDiZTsYJo3"},"outputs":[],"source":["param_lims = {1000000: '1M', 10000000: '10M'}\n","layer_range = [2,6,10]\n","num_models = 10\n","\n","'''\n","var = 'tas'\n","var_idx = vars_to_predict.index(var)\n","\n","X_train = X_train_dict[var]\n","Y_train = Y_train_dict[var]\n","X_val = X_val_dict[var]\n","Y_val = Y_val_dict[var]\n","'''\n","vars_to_predict = ['tas', 'diurnal_temperature_range', 'pr', 'pr90']\n","\n","rmse_diffs = []\n","rmse_diffs_orig = []\n","\n","rmse_rand_model_1M_raw = []\n","rmse_rand_model_10M_raw = []\n","rmse_frozen_model_1M_raw = []\n","rmse_frozen_model_10M_raw = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GpilALphg4cI"},"outputs":[],"source":["cnn_lstm_1M_total = np.load('./drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm/new_metrics_experiment/1M/rmse_data_total.npy')\n","cnn_lstm_10M_total = np.load('./drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm/new_metrics_experiment/10M/rmse_data_total.npy')\n","cnn_lstm_1M_models_loc = './drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm/new_metrics_experiment/1M/models/'\n","cnn_lstm_10M_models_loc = './drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm/new_metrics_experiment/10M/models/'\n","\n","cnn_lstm_rand_dense_1M_total = np.load('./drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm_rand_dense/new_metrics_experiment/1M/rmse_data_total.npy')\n","cnn_lstm_rand_dense_10M_total = np.load('./drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm_rand_dense/new_metrics_experiment/10M/rmse_data_total.npy')\n","cnn_lstm_rand_dense_1M_models_loc = './drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm_rand_dense/new_metrics_experiment/1M/models/'\n","cnn_lstm_rand_dense_10M_models_loc = './drive/My Drive/Colab Notebooks/USC Random NN/experimental_results/cnn_lstm_rand_dense/new_metrics_experiment/10M/models/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RQz-tA3Jibq4"},"outputs":[],"source":["for param_lim in param_lims.keys():\n","\n","  if param_lim == 1000000:\n","    dense_rmse_data = cnn_lstm_1M_total\n","    dense_models_loc = cnn_lstm_1M_models_loc\n","    rand_dense_rmse_data = cnn_lstm_rand_dense_1M_total\n","    rand_dense_models_loc = cnn_lstm_rand_dense_1M_models_loc\n","  else:\n","    dense_rmse_data = cnn_lstm_10M_total\n","    dense_models_loc = cnn_lstm_10M_models_loc\n","    rand_dense_rmse_data = cnn_lstm_rand_dense_10M_total\n","    rand_dense_models_loc = cnn_lstm_rand_dense_10M_models_loc\n","\n","  for n_layers in layer_range:\n","\n","      idx = n_layers - 2\n","\n","      for i in range(num_models):\n","\n","        for var in vars_to_predict:\n","          \n","          var_idx = vars_to_predict.index(var)\n","\n","          keras.backend.clear_session()\n","          stand_model = None\n","          rand_model = None\n","          new_model = None\n","\n","          # Get train/val data\n","          X_train = X_train_dict[var]\n","          Y_train = Y_train_dict[var]\n","          X_val = X_val_dict[var]\n","          Y_val = Y_val_dict[var]\n","\n","          # Get best standard model\n","          best_stand_model_idx = np.argmin(dense_rmse_data[idx,var_idx])\n","          config = np.load(f'{dense_models_loc}{n_layers}_layer_model_{best_stand_model_idx}.npy', allow_pickle=True)\n","          stand_model = Sequential.from_config(config.item())\n","          #stand_model.summary()\n","\n","          # Train best standard model\n","          stand_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n","          hist = stand_model.fit(X_train,\n","                                 Y_train,\n","                                 batch_size=16,\n","                                 epochs=100,\n","                                 verbose=0,\n","                                 validation_data=(X_val,Y_val),\n","                                 callbacks=EarlyStopping(patience=10, restore_best_weights=True),\n","                                )\n","          \n","          # Get best random model\n","          best_rand_model_idx = np.argmin(rand_dense_rmse_data[idx,var_idx])\n","          config = np.load(f'{rand_dense_models_loc}{n_layers}_layer_model_{best_rand_model_idx}.npy', allow_pickle=True)\n","          custom_objects = {\"ApplyPosWeight\": ApplyPosWeight}\n","          with tf.keras.utils.custom_object_scope(custom_objects):\n","            rand_model = Model.from_config(config.item())\n","          #rand_model.summary()\n","\n","          # Get the random layers from the best random model\n","          rand_layers = [layer.__class__.__name__ for layer in rand_model.layers]\n","          rand_start_idx = rand_layers.index('TFOpLambda')\n","          rand_block = Model(rand_model.layers[rand_start_idx-1].output, rand_model.layers[-1].output)\n","          #rand_block.summary()\n","\n","          # Get non-random layers from standard model, add random layers on top to create new model\n","          stand_layers = [layer.__class__.__name__ for layer in stand_model.layers]\n","          stand_end_idx = stand_layers.index('Dense')-1\n","          new_output = rand_block(stand_model.layers[stand_end_idx].output)\n","          new_model = Model(stand_model.input, new_output)\n","          #new_model.summary()\n","\n","          # Freeze non-random layers of new model\n","          for j in range(rand_start_idx):\n","            new_model.layers[j].trainable = False\n","\n","          # Train and evaluate best random model\n","          rand_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n","          hist = rand_model.fit(X_train,\n","                                Y_train,\n","                                batch_size=16,\n","                                epochs=100,\n","                                verbose=0,\n","                                validation_data=(X_val,Y_val),\n","                                callbacks=EarlyStopping(patience=10, restore_best_weights=True),\n","                               )\n","          m_pred = rand_model.predict(X_test_np)\n","          m_pred = m_pred.reshape(m_pred.shape[0], m_pred.shape[2], m_pred.shape[3])\n","          m_pred = xr.DataArray(m_pred, dims=['time','lat','lon'], coords=[X_test.time.data[slider-1:], X_test.latitude.data, X_test.longitude.data])\n","          m_pred = m_pred.transpose('lat','lon','time').sel(time=slice(2015,2101)).to_dataset(name=var)\n","          var_truth = Y_test[var]\n","          m_var_pred = m_pred.transpose('time','lat','lon')[var]\n","          rmse_spatial = get_rmse_spatial(var_truth[65:], m_var_pred[65:])\n","          rmse_global = get_rmse_global(var_truth[65:], m_var_pred[65:])\n","          rmse_total = rmse_spatial + 5*rmse_global\n","          if param_lim == 1000000:\n","            rmse_rand_model_1M_raw.append(rmse_total)\n","          else:\n","            rmse_rand_model_10M_raw.append(rmse_total)\n","\n","          # Train and evaluate new model (frozen weights in non-random layers)\n","          new_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n","          hist = new_model.fit(X_train,\n","                               Y_train,\n","                               batch_size=16,\n","                               epochs=100,\n","                               verbose=0,\n","                               validation_data=(X_val,Y_val),\n","                               callbacks=EarlyStopping(patience=10, restore_best_weights=True),\n","                              )\n","          m_pred = new_model.predict(X_test_np)\n","          m_pred = m_pred.reshape(m_pred.shape[0], m_pred.shape[2], m_pred.shape[3])\n","          m_pred = xr.DataArray(m_pred, dims=['time','lat','lon'], coords=[X_test.time.data[slider-1:], X_test.latitude.data, X_test.longitude.data])\n","          m_pred = m_pred.transpose('lat','lon','time').sel(time=slice(2015,2101)).to_dataset(name=var)\n","          var_truth = Y_test[var]\n","          m_var_pred = m_pred.transpose('time','lat','lon')[var]\n","          rmse_spatial = get_rmse_spatial(var_truth[65:], m_var_pred[65:])\n","          rmse_global = get_rmse_global(var_truth[65:], m_var_pred[65:])\n","          rmse_total = rmse_spatial + 5*rmse_global\n","          if param_lim == 1000000:\n","            rmse_frozen_model_1M_raw.append(rmse_total)\n","          else:\n","            rmse_frozen_model_10M_raw.append(rmse_total)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oOrpvd-dA5qI"},"outputs":[],"source":["# Some data reshaping for easier analysis\n","rmse_rand_model_1M_raw = np.array(rmse_rand_model_1M_raw)\n","rmse_rand_model_1M_raw = np.reshape(rmse_rand_model_1M_raw, (len(layer_range),len(vars_to_predict)*num_models))\n","rmse_rand_model_1M = np.zeros((len(layer_range),len(vars_to_predict),num_models))\n","for i in range(rmse_rand_model_1M_raw.shape[0]):\n","  rmse_rand_model_1M[i] = np.reshape(rmse_rand_model_1M_raw[i], (len(vars_to_predict),num_models), order='F')\n","\n","rmse_rand_model_10M_raw = np.array(rmse_rand_model_10M_raw)\n","rmse_rand_model_10M_raw = np.reshape(rmse_rand_model_10M_raw, (len(layer_range),len(vars_to_predict)*num_models))\n","rmse_rand_model_10M = np.zeros((len(layer_range),len(vars_to_predict),num_models))\n","for i in range(rmse_rand_model_10M_raw.shape[0]):\n","  rmse_rand_model_10M[i] = np.reshape(rmse_rand_model_10M_raw[i], (len(vars_to_predict),num_models), order='F')\n","\n","rmse_frozen_model_1M_raw = np.array(rmse_frozen_model_1M_raw)\n","rmse_frozen_model_1M_raw = np.reshape(rmse_frozen_model_1M_raw, (len(layer_range),len(vars_to_predict)*num_models))\n","rmse_frozen_model_1M = np.zeros((len(layer_range),len(vars_to_predict),num_models))\n","for i in range(rmse_frozen_model_1M_raw.shape[0]):\n","  rmse_frozen_model_1M[i] = np.reshape(rmse_frozen_model_1M_raw[i], (len(vars_to_predict),num_models), order='F')\n","\n","rmse_frozen_model_10M_raw = np.array(rmse_frozen_model_10M_raw)\n","rmse_frozen_model_10M_raw = np.reshape(rmse_frozen_model_10M_raw, (len(layer_range),len(vars_to_predict)*num_models))\n","rmse_frozen_model_10M = np.zeros((len(layer_range),len(vars_to_predict),num_models))\n","for i in range(rmse_frozen_model_10M_raw.shape[0]):\n","  rmse_frozen_model_10M[i] = np.reshape(rmse_frozen_model_10M_raw[i], (len(vars_to_predict),num_models), order='F')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"p5oAWwKgVfHW"},"outputs":[],"source":["np.save(results_path+f'rmse_rand_model_1M', rmse_rand_model_1M)\n","np.save(results_path+f'rmse_rand_model_10M', rmse_rand_model_10M)\n","np.save(results_path+f'rmse_frozen_model_1M', rmse_frozen_model_1M)\n","np.save(results_path+f'rmse_frozen_model_10M', rmse_frozen_model_10M)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMUqP7WfdIIFqRw66uPPvGY"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}